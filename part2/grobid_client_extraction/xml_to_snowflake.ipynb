{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../resources/grobid/Level1_combined.grobid.tei.xml\n",
      "../resources/grobid/Level2_combined.grobid.tei.xml\n",
      "../resources/grobid/Level3_combined.grobid.tei.xml\n",
      "Combined CSV file '../resources/grobid/outcomes.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# List of XML file paths\n",
    "xml_file_paths = ['../resources/grobid/Level1_combined.grobid.tei.xml', '../resources/grobid/Level2_combined.grobid.tei.xml','../resources/grobid/Level3_combined.grobid.tei.xml']\n",
    "\n",
    "# Create a CSV file\n",
    "csv_filename = '../resources/grobid/outcomes.csv'\n",
    "\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write header\n",
    "    csv_writer.writerow(['Topic', 'Learning Outcomes Section'])\n",
    "    \n",
    "    # Iterate through each XML file\n",
    "    for xml_file_path in xml_file_paths:\n",
    "        print(xml_file_path)\n",
    "        # Read the XML data from the file\n",
    "        with open(xml_file_path, 'r') as file:\n",
    "            xml_data = file.read()\n",
    "        # Parse the XML data\n",
    "        root = ET.fromstring(xml_data)\n",
    "\n",
    "        for div_element in root.findall('.//{http://www.tei-c.org/ns/1.0}div'):\n",
    "                head_element = div_element.find('.//{http://www.tei-c.org/ns/1.0}head')\n",
    "\n",
    "                p_elements = div_element.findall('.//{http://www.tei-c.org/ns/1.0}p')\n",
    "\n",
    "                combined_p_text = ' '.join(p_element.text for p_element in p_elements if p_element.text)\n",
    "                if combined_p_text != '':\n",
    "                    csv_writer.writerow([head_element.text if head_element is not None else '', combined_p_text])\n",
    "\n",
    "print(f\"Combined CSV file '{csv_filename}' created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jr53964.us-east-2.aws\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv('snowflake_account'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x10940f950>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "snowflake_account = os.getenv('snowflake_account')\n",
    "snowflake_user = os.getenv('snowflake_user')\n",
    "snowflake_password = os.getenv('snowflake_password')\n",
    "snowflake_warehouse = os.getenv('snowflake_warehouse')\n",
    "snowflake_schema = os.getenv('snowflake_schema')\n",
    "\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    account=snowflake_account,\n",
    "    warehouse=snowflake_warehouse,\n",
    "    schema=snowflake_schema\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "target_database = 'CFAInstitute'\n",
    "target_table = 'Learning_Outcomes'\n",
    "\n",
    "create_database_query = f\"CREATE DATABASE IF NOT EXISTS {target_database}\"\n",
    "cursor.execute(create_database_query)\n",
    "\n",
    "use_database_query = f\"USE DATABASE {target_database}\"\n",
    "cursor.execute(use_database_query)\n",
    "cursor.execute(\"USE WAREHOUSE TEST\")\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_table} (\n",
    "    \"Learning Outcomes Section\" VARCHAR,\n",
    "    Topic VARCHAR\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(f\"TRUNCATE TABLE {target_table}\")\n",
    "\n",
    "# cursor.execute(f\"PUT file://combined_output.csv @%{target_table}\")\n",
    "\n",
    "# cursor.execute(f\"COPY INTO {target_table} ON_ERROR=CONTINUE FILE_FORMAT = (FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' SKIP_HEADER=1 PARSE_HEADER = FALSE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and upload to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespaces(value):\n",
    "    if isinstance(value, str):\n",
    "        return ' '.join(value.split())\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def preprocess_text(value):\n",
    "    if isinstance(value, str):\n",
    "        return f'\"{value}\"'\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to s3 first now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ../resources/grobid/out/Grobid_RR_2024_Level3_combined.txt to s3://cfainstitute-learning-outcomes-raw/grobid/Grobid_RR_2024_Level3_combined.txt\n",
      "Uploading ../resources/grobid/out/Grobid_RR_2024_Level2_combined.txt to s3://cfainstitute-learning-outcomes-raw/grobid/Grobid_RR_2024_Level2_combined.txt\n",
      "Uploading ../resources/grobid/out/Grobid_RR_2024_Level1_combined.txt to s3://cfainstitute-learning-outcomes-raw/grobid/Grobid_RR_2024_Level1_combined.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def upload_folder_to_s3(local_folder_path, s3_bucket_name, s3_folder_path, aws_access_key_id, aws_secret_access_key):\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "    # Iterate through all the files in the local folder\n",
    "    for root, dirs, files in os.walk(local_folder_path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_key = os.path.relpath(local_file_path, local_folder_path)\n",
    "            s3_object_key = os.path.join(s3_folder_path, s3_key).replace(\"\\\\\", \"/\")  # Replace backslashes with forward slashes for cross-platform compatibility\n",
    "            \n",
    "            # Upload each file to S3\n",
    "            print(f\"Uploading {local_file_path} to s3://{s3_bucket_name}/{s3_object_key}\")\n",
    "            s3.upload_file(local_file_path, s3_bucket_name, s3_object_key)\n",
    "\n",
    "local_folder_path = '../resources/grobid/out'\n",
    "aws_access_key_id = os.getenv('aws_access_key_id')\n",
    "aws_secret_access_key = os.getenv('aws_secret_access_key')\n",
    "bucket_name = 'cfainstitute-learning-outcomes-raw'\n",
    "s3_folder_path = 'grobid'\n",
    "upload_folder_to_s3(local_folder_path, bucket_name, s3_folder_path, aws_access_key_id, aws_secret_access_key,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload outcomes.csv to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been uploaded to S3 at s3://cfainstitute-learning-outcomes-raw/outcomes_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../resources/grobid/outcomes.csv', dtype=str)\n",
    "df = df.map(remove_extra_whitespaces)\n",
    "df = df.map(preprocess_text)\n",
    "\n",
    "df.to_csv('../resources/grobid/outcomes_processed.csv', index=False)\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "csv_data = df.to_csv(index=False)\n",
    "\n",
    "s3_key = 'outcomes_processed.csv'\n",
    "\n",
    "s3.put_object(Body=csv_data, Bucket=bucket_name, Key=s3_key)\n",
    "\n",
    "print(f\"CSV file has been uploaded to S3 at s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create file format first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x10940f950>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"CREATE OR REPLACE FILE FORMAT mycsvformat\n",
    "   TYPE = 'CSV'\n",
    "   FIELD_DELIMITER = '|'\n",
    "   SKIP_HEADER = 1;\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Goto iam of aws and create a role\n",
    " use above role arn to create storage integration\n",
    " Create STORAGE INTEGRATION that can connect to aws account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x12a118490>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cursor.execute(\"\"\"CREATE OR REPLACE STORAGE INTEGRATION s3_int2\n",
    "#   TYPE = EXTERNAL_STAGE\n",
    "#   STORAGE_PROVIDER = 'S3'\n",
    "#   STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::640055273174:role/s3-read'\n",
    "#   ENABLED = TRUE\n",
    "#   STORAGE_ALLOWED_LOCATIONS = ('*')\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " go to trusted relationships of iam role created in above step from aws console and change it to below template\n",
    "\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"s3.amazonaws.com\",\n",
    "                \"AWS\": \"<user arn>\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"sts:ExternalId\": \"<external id>\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "run \"DESC Integration s3_int2\"  inside snowflake to get STORAGE_AWS_IAM_USER_ARN and STORAGE_AWS_EXTERNAL_ID of storage integration crreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x10940f950>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"CREATE OR REPLACE STAGE external_stage\n",
    "  FILE_FORMAT = mycsvformat\n",
    "  URL = 's3://cfainstitute-learning-outcomes-raw/outcomes.csv'\n",
    "  STORAGE_INTEGRATION = s3_int2;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(f\"\"\"\n",
    "# COPY INTO {target_table}\n",
    "#   FROM @external_stage\n",
    "#   ON_ERROR = 'CONTINUE';\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_format = (type = csv skip_header = 1  PARSE_HEADER = FALSE field_delimiter = ',');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to snowflake from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x10940f950>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "copy into Learning_Outcomes\n",
    "from 's3://cfainstitute-learning-outcomes-raw/outcomes_processed.csv'\n",
    "storage_integration = s3_int2\n",
    "FORCE = TRUE\n",
    "ON_ERROR = CONTINUE\n",
    "  file_format = (type = csv FIELD_OPTIONALLY_ENCLOSED_BY = '\"' SKIP_HEADER=1 PARSE_HEADER = FALSE);\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
