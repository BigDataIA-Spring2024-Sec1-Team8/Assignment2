{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time,csv\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(link):\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extracting data using BeautifulSoup\n",
    "        title = soup.find(class_='article-title').text.strip() if soup.find(class_='article-title') else ''\n",
    "        curriculum = soup.find(class_='content-utility-curriculum').text.strip() if soup.find(class_='content-utility-curriculum') else ''\n",
    "        level = soup.find(class_='content-utility-level').text.strip() if soup.find(class_='content-utility-level') else ''\n",
    "        topics = soup.find(class_='content-utility-topics').text.strip() if soup.find(class_='content-utility-topics') else ''\n",
    "\n",
    "        # Extracting data after the \"Learning Outcomes\" section\n",
    "        learning_outcomes_section = soup.find('h2', text='Learning Outcomes')\n",
    "        if not (learning_outcomes_section == None):\n",
    "            section_text = learning_outcomes_section.find_next('section').text.strip()\n",
    "        else:\n",
    "            section_text = \"\"\n",
    "\n",
    "        introduction_section = soup.find('h2', text='Introduction') or soup.find('h2', string='Overview')\n",
    "        if not (introduction_section == None):\n",
    "            introduction_section_text = introduction_section.find_next('section').text.strip()\n",
    "        else:\n",
    "            introduction_section_text = \"\"\n",
    "        summary_section = soup.find('h2', text='Summary')\n",
    "        summary_bullets = []\n",
    "        if not(summary_section==None):\n",
    "            ul = summary_section.find_next('ul')\n",
    "            if ul:\n",
    "                summary_bullets = [li.text.strip() for li in ul.find_all('li')]\n",
    "        pdf_link=\"\"\n",
    "        a = soup.find('a', string='Download the full reading (PDF)') \n",
    "        if a:\n",
    "            pdf_link = a['href']\n",
    "        return {\n",
    "            'Title': title,\n",
    "            'Curriculum': curriculum,\n",
    "            'Level': level,\n",
    "            'Topics': topics,\n",
    "            'Learning Outcomes Section': section_text,\n",
    "            'Introduction': introduction_section_text,\n",
    "            'Summary Bullets': ', '.join(summary_bullets),\n",
    "            \"pdf_link\": pdf_link\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {link}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(output_data):\n",
    "    fieldnames = ['Title', 'Curriculum', 'Level', 'Topics', 'Learning Outcomes Section', 'Introduction', 'Summary Bullets', 'pdf_link']\n",
    "    csv_output_path = './resources/raw_content.csv'\n",
    "    # Write data to the CSV file\n",
    "    with open(csv_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        \n",
    "        for data in output_data:\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(path):\n",
    "    aws_access_key_id = os.getenv('aws_access_key_id')\n",
    "    aws_secret_access_key = os.getenv('aws_secret_access_key')\n",
    "    bucket_name = 'cfainstitute-topic-details-raw'\n",
    "    s3_key = 'raw_content.csv'  \n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "    csv_data = df.to_csv(index=False)\n",
    "    s3.put_object(Body=csv_data, Bucket=bucket_name, Key=s3_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content():\n",
    "    csv_file_path = './resources/links.csv' \n",
    "    output_data = []\n",
    "\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  \n",
    "        for row in csv_reader:\n",
    "            link = row[0]  \n",
    "            data = extract_data(link)\n",
    "            if data:\n",
    "                output_data.append(data)\n",
    "    \n",
    "    write_to_csv(output_data=output_data)\n",
    "    upload_to_s3(path='./resources/raw_content.csv')\n",
    "\n",
    "extract_content()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
